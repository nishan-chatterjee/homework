{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a', 'banana', 'a', 'day', 'keeps', 'the', 'doctor', 'away'], ['i', 'would', 'rather', 'have', 'a', 'sweet', 'chocolate', 'cookie', 'now'], ['banana', 'banana', 'banana']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "dataset = [\"A banana a day keeps the doctor away.\",\n",
    "           \"I would rather have a sweet chocolate cookie now.\", \"Banana!  Banana!  Banana!\"]\n",
    "data_tokenized = []\n",
    "for data in dataset:\n",
    "    words = nltk.word_tokenize(data)\n",
    "    data_tokenized.append([word.lower() for word in words if word.isalpha()])\n",
    "print(data_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 0, 0], [0, 1, 1, 1], [3, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "v = {'v0': 'banana', 'v1': 'chocolate', 'v2': 'sweet', 'v3': 'cookie'}\n",
    "data_vector = []\n",
    "for data in data_tokenized:\n",
    "    data_vector.append([data.count(v['v0']), data.count(v['v1']), data.count(v['v2']), data.count(v['v3'])])\n",
    "print(data_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters are:  6051\n"
     ]
    }
   ],
   "source": [
    "input_layer = 4\n",
    "hidden_layer_1 = 200\n",
    "hidden_layer_2 = 25\n",
    "output_layer = 1\n",
    "\n",
    "input_layer1 = input_layer*hidden_layer_1 + hidden_layer_1\n",
    "layer1_layer2 = hidden_layer_1*hidden_layer_2 + hidden_layer_2\n",
    "layer2_output = hidden_layer_2*output_layer + output_layer\n",
    "\n",
    "total_parameters = input_layer1 + layer1_layer2 + layer2_output\n",
    "\n",
    "print(\"The total number of parameters are: \", total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Output:  1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "tweet1 = np.array([data_vector[0]])\n",
    "input_layer = tweet1.transpose()\n",
    "\n",
    "# parameter settings\n",
    "hidden1 = np.array([[-4, -3, 3, 1], [-5, 5, -1, -5], [5, -5, 4, 1]])\n",
    "bias1 = np.array([[0], [0], [0]])\n",
    "\n",
    "hidden2 = np.array([[-1, -1, 2], [5, -3, -5]])\n",
    "bias2 = np.array([[1], [3]])\n",
    "\n",
    "hidden3 = np.array([[3, -1]])\n",
    "bias3 = np.array([[0]])\n",
    "\n",
    "# computing layers\n",
    "layer1 = np.dot(hidden1, input_layer) + bias1\n",
    "layer2 = np.dot(hidden2, layer1) + bias2\n",
    "layer3 = np.dot(hidden3, layer2) + bias3\n",
    "\n",
    "# after using sigmoid activation\n",
    "network_output = 1/(1 + np.exp(-layer3[0][0]))\n",
    "print(\"Network Output: \", network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new output looks like:  [[-121]\n",
      " [-114]\n",
      " [-114]]\n",
      "The Softmax Prediction [[0. ]\n",
      " [0.5]\n",
      " [0.5]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "# for tweet 1\n",
    "tweet1 = np.array([data_vector[0]]).transpose()\n",
    "\n",
    "# parameter settings\n",
    "hidden1 = np.array([[-4, -3, 3, 1], [-5, 5, -1, -5], [5, -5, 4, 1]])\n",
    "bias1 = np.array([[0], [0], [0]])\n",
    "\n",
    "hidden2 = np.array([[-1, -1, 2], [5, -3, -5]])\n",
    "bias2 = np.array([[1], [3]])\n",
    "\n",
    "hidden3 = np.array([[-2, 3], [-3, 2], [-3, 2]])\n",
    "bias3 = np.array([[0], [0], [0]])\n",
    "\n",
    "# computing layers\n",
    "tweet1_layer1 = np.dot(hidden1, tweet1) + bias1\n",
    "tweet1_layer2 = np.dot(hidden2, tweet1_layer1) + bias2\n",
    "tweet1_layer3 = np.dot(hidden3, tweet1_layer2) + bias3\n",
    "\n",
    "# layer outputs\n",
    "# print(tweet1_layer1)\n",
    "# print(tweet1_layer2)\n",
    "# print(tweet1_layer3)\n",
    "\n",
    "print(\"The new output looks like: \", tweet1_layer3)\n",
    "print(\"The Softmax Prediction\", np.around(softmax(tweet1_layer3), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new output looks like:  [[31]\n",
      " [19]\n",
      " [19]]\n",
      "The Softmax Prediction [[1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# For tweet2\n",
    "\n",
    "tweet2 = np.array([data_vector[1]]).transpose()\n",
    "\n",
    "# parameter settings\n",
    "hidden1 = np.array([[-4, -3, 3, 1], [-5, 5, -1, -5], [5, -5, 4, 1]])\n",
    "bias1 = np.array([[0], [0], [0]])\n",
    "\n",
    "hidden2 = np.array([[-1, -1, 2], [5, -3, -5]])\n",
    "bias2 = np.array([[1], [3]])\n",
    "\n",
    "hidden3 = np.array([[-2, 3], [-3, 2], [-3, 2]])\n",
    "bias3 = np.array([[0], [0], [0]])\n",
    "\n",
    "# computing layers\n",
    "tweet2_layer1 = np.dot(hidden1, tweet2) + bias1\n",
    "tweet2_layer2 = np.dot(hidden2, tweet2_layer1) + bias2\n",
    "tweet2_layer3 = np.dot(hidden3, tweet2_layer2) + bias3\n",
    "\n",
    "# layer outputs\n",
    "# print(tweet2_layer1)\n",
    "# print(tweet2_layer2)\n",
    "# print(tweet2_layer3)\n",
    "\n",
    "print(\"The new output looks like: \", tweet2_layer3)\n",
    "print(\"The Softmax Prediction\", np.around(softmax(tweet2_layer3), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_true = [1, 0, 0]\n",
    "y_pred = [0.97, 0.02, 0.00001]\n",
    "print(\"MSE: \", np.around(mean_squared_error(y_true, y_pred), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy/Log Loss:  0.017\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "print(\"Cross Entropy/Log Loss: \", np.around(log_loss(y_true, y_pred), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
